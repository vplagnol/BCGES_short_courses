\documentclass[a4paper]{article}
\usepackage[colorlinks=true]{hyperref}
\usepackage{fullpage}

\title{BCGES short courses, session 1: Introduction, file formats, shell scripting, Galaxy}
\date{}
\author{Vincent Plagnol}

\begin{document}
%\SweaveOpts{concordance=TRUE}
%knit('practical.Rnw'); system('pdflatex practical')

\maketitle

<<setup, results = 'hide', echo = FALSE>>=
if (!exists('forStudents')) forStudents <- TRUE  ##default is to print the student version
if (!forStudents) {echo <- TRUE; results <- 'markup'; fig.show <- 'asis'} else {results <- 'hide'; echo  <- FALSE; fig.show <- 'hide';}
options(tidy=TRUE, width=80)
@

The aim of this first session is to introduce the tools that we will use to manipulate sequence data.
Generally speaking, we will deal with either \texttt{R}, bash scripting or \href{https://wiki.galaxyproject.org/PublicGalaxyServers}{Galaxy}.


\texttt{R} is not usually the preferred way to deal with HTS data.
One reason for this is that the general approach of \texttt{R} is to load data into the RAM as a first instance, which is not practical when the datasets are very large.
Shell scripts (aka command line tools) that read the data on a line per line basis are usually preferred.
Nevertheless, \texttt{R} has developed tools to overcome these limitations and there is, in fact, surprisingly much that one can do with it.
It is also a practical tools for teaching purposes, which is useful in the context of these short courses.
So we will use \texttt{R} mostly to play with the data and show the sort of things one may want to do with it.

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Basic fastq reading and quality control (1h)}


\subsection{Basic shell scripting to read NGS files (20 minutes)}


Many of the standard HTS formats are simple text files, with tightly defined specifications to allow effective parsing.
Some of these files can be compressed and/or indexed to enable quicker access.
The first esstential step is to be confortable with reading/compressing/uncompressing various text files.

<<head, engine='bash'>>=
head ../../data/fastq_files/fastq1_1.txt
@ 


<<tail, engine='bash'>>=
tail ../../data/fastq_files/fastq1_1.txt
@ 

If you need more information, shell scripts have useful manual page that can be accessed using the \texttt{man} command.
<<man, engine='bash', eval=FALSE>>=
man head
man tail
@ 

One can parse a text file using a variety of commands, and it is useful to become familiar with the following:
<<reads, engine='bash', eval = FALSE>>=
cat ../data/fastq_files/fastq1_1.txt
less ../data/fastq_files/fastq1_1.txt
more ../data/fastq_files/fastq1_1.txt
less -S ../data/fastq_files/fastq1_1.txt
@ 

\noindent
{\bf Exercise:} Can you see the differences between these different ways of reading a file?

\vspace{1.5cm}


A routine that is often useful is \texttt{wc} that counts the words/character/lines of a file. Try:
<<wc, engine='bash', eval= FALSE>>=
wc  ../data/fastq_files/fastq1_1.txt
wc  -l ../data/fastq_files/fastq1_1.txt
@


\noindent
{\bf Exercise:} Using the man page, find a way to print the first 20 lines of a fastq file (and what about the last 20 lines)?\\
{\bf Exercise:} Based on the \texttt{wc} output, how many reads do these fastq files contain?

<<<wc.answers, engine='bash', eval= TRUE, echo = echo, results = results>>=
## Some answers to the two questions above:
head -20 ../data/fastq_files/fastq1_1.txt > first_20.txt
tail -20 ../data/fastq_files/fastq1_1.txt > last_20.txt
## each read uses 4 lines, so the output of wc -l must be divided by 4 to get the read count.
@ >>=


\subsection{Using the shortRead package in R (15 minutes)}
We start by loading one of the most relevant library, called ``ShortReads''. This package may not be installed but it can easily done so by running:
<<install, eval = FALSE>>=
source("http://bioconductor.org/biocLite.R")
biocLite("ShortRead")
@ 

<<load, results = 'hide', message = FALSE>>=
library('ShortRead')
@ 

As a starting point it is possible to read some of the examples fastq files and create relevant \texttt{R} objects.
<<readFastq, eval=TRUE>>=
fastq1.1 <- readFastq('../data/fastq_files/fastq1_1.txt')
fastq1.2 <- readFastq('../data/fastq_files/fastq1_2.txt')
@ 

We can now display the sequences and the qualities. Note that specific classes have been defined to store each of these objects.
A lot of work has gone into figuring out how to do this.

<<showFastq>>=
reads <- sread(fastq1.1)
class(reads)
head(as.character(reads))

ids <- id(fastq1.1)
class(ids)
head(as.character(ids))
@ 

It is also possible to use this package to look at quality scores.
For example, following up on the example above:

<<showQuals>>=
quals <- quality(fastq1.1)
class(quals)
quals
@ 


The \texttt{ShortRead} package will attempt to guess what these quality scores mean, for example see:
<<guessQual>>=
encoding(quality(fastq1.1))

fastq2.1 <- readFastq('../data/fastq_files/fastq2_1.txt')
encoding(quality(fastq2.1))
@ 

\noindent
{\bf (Optional, and somewhat difficult) Exercise:} Generate a plot showing the average Phred score as a function of the position in the read.

<<plotQual, eval = TRUE, results = results, echo = echo, fig.show = fig.show, fig.width = 7, fig.height = 7>>=
## Here is how I would create that plot
ave.qual <- apply(FUN = mean, MAR = 2, as(quals, 'matrix'))
plot (x = 1:length(ave.qual),
      y = ave.qual,
      xlab = 'Position in read',
      ylab = 'Average Phred score')
@ 

  
\subsection{Using the Galaxy server (30 minutes)}

Start by identifying a working instance of the Galaxy server from this \href{https://wiki.galaxyproject.org/PublicGalaxyServers}{location}.
There are multiple choices here, so feel free to experiment.
In case of doubt, I propose that you use the \href{https://usegalaxy.org/}{main Galaxy instance}.
However, I have used the following: \href{http://biominavm-galaxy.biomina.be/galaxy/}{http://biominavm-galaxy.biomina.be/galaxy/}, and it worked for me.
However, be ready for small and subtle differences and be flexible.

\noindent
{\bf Exercise:} Create an account on one of these Galaxy servers and upload the pair of fastq {\it fastq2\_1.txt} and {\it fastq2\_2.txt}.
Run the \texttt{fastqc} code on these files (note that a standalone version also exists and can be found \href{http://www.bioinformatics.babraham.ac.uk/projects/fastqc/}{here}).

<<tips1, tidy = TRUE, echo = echo, results = results>>=
#A difficulty is that the two fastq files need to be joined together to mark the fact that they are paired end reads.
#There is a tool to do this called FASTQ joiner.
#And if FASTQ joiner does not work, it is probably because you need to run FASTQ Groomer first, which does some magic to ``prepare'' the fastq files.
#But these things may differ for different instances of the Galaxy server. Be flexible!
@ 

\noindent
{\bf (Optional) Exercise:} Run \texttt{fastqc} locally (after downloading it from the  \href{http://www.bioinformatics.babraham.ac.uk/projects/fastqc/}{web}) and compare the output with what you get from Galaxy (it should be identical!).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Merging overlapping paired reads reads (20 minutes)}

Another useful thing to be aware of is the possibility to merge read pairs that overlap in the middle.
This happens when the combined length of both reads exceeds the length of the DNA fragment they originate from.
A review of tools to perform this task is available \href{http://thegenomefactory.blogspot.de/2012/11/tools-to-merge-overlapping-paired-end.html}{here}.
I propose here to have a look at the program \href{http://sco.h-its.org/exelixis/web/software/pear/doc.html}{PEAR}, which has been useful for the applications I had in mind.
Start by downloading the pre-compiled binary to your own computer (this program is not preinstalled).
You should be able to run something like the command below, after changing the path to the appropriate location of the executable:

<<pear, engine = 'bash', eval = FALSE>>=
bin/pear-0.9.4-64 -e -v 10 \
   -f ../data/fastq_for_merging/reads1.fq -r ../data/fastq_for_merging/reads1.fq -o merged -c 40
@

Check what the output looks like, in particular the read length of the assembled reads. 
Have a look at the options that PEAR uses, and see how the choice of the \texttt{v} argument affects the final results.

\noindent
{\bf Warning:} Note that PEAR requires that you know which of the reads are forward and reverse, which is probably not the case for a fastq file that is not stranded or that has been processed. So be a bit careful before running this code on a generic fastq.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Reading and manipulating BAM files (about 1.5h)}
BAM files are compressed files that contain the information from the FASTQ files, plus additional information about the location where the reads map.
A key feature of the BAM files is that they can be indexed, i.e. an associated file contains information about where each of the reads are located in the file.
It allows very quick retrieval of reads that map to a given genomic location, which is the typical way one wants to use BAM files (for example, to extract all the reads that map to a gene of interest in order to find rare variants).

\subsection{Using shell scripts ans \texttt{samtools} (40 minutes)}

\texttt{samtools} is the key piece of software that is used to read, write and index BAM files.
The \href{http://samtools.sourceforge.net/samtools.shtml}{manual page} is the first place to go to find information about how to use \texttt{samtools}.
A very useful feature of \texttt{samtools} is that it can work over a ftp site, by downloading the index locally and only pulling the reads that are relevant.
That allows to access rich datasets online without having to download very large files.
The exercise below illustrates some of these features.

\noindent
{\bf Exercise:} The following should be useful as an introduction to the sort of things one may want to do with \texttt{samtools}.
The manual page should have all the commands and ideas to go through these, so best to have a go and try.


\begin{enumerate}
\item Download the index of all the aligned file from the 1,000 Genomes. You can for example do it using:
<<wget, engine = 'bash', eval = TRUE>>=
wget -O 1KG_index.tab ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/exome.alignment.index
@ 
\item Using \texttt{samtools view} over the web, download the {\it BRCA1} reads for the sample \texttt{HG00118}.
  You will first need to find where that file is located on the web, using the index you just downloaded.
  Make sure that your output is in BAM format.
<<view, engine = 'bash', echo = echo, results = results>>=
##first I grep the index file to find the filename
grep HG00118 1KG_index.tab  | grep HG00118.mapped.ILLUMINA | awk '{print $1}'
BAM=ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data/HG00118/exome_alignment/HG00118.mapped.ILLUMINA.bwa.GBR.exome.20130415.bam
##Then samtools view, note the -b option to generate a BAM output
samtools view -b -o BRCA1_HG00118.bam $BAM 17:41243452-41277500
@ 
\item Index this BAM file using \texttt{samtools index}.
<<index, engine = 'bash', echo = echo, results = results>>=
##Basic index command below, a must know!
samtools index BRCA1_HG00118.bam
@ 
\item Still using \texttt{samtools view}, subset the BAM file for the first coding exon (not the UTR) of {\it BRCA1}  (use transcript ENST00000357654 from Ensembl) and output in SAM format.
<<exon1, engine = 'bash', echo = echo, results = results>>=
## the line of code below should create that SAM file
samtools view BRCA1_HG00118.bam 17:43124115-43124017  > BRCA1_exon1.sam
@ 
\item Exercise: What do these lines do?
<<view.2, engine = 'bash', eval = FALSE>>=
samtools view -f 0x0002 BRCA1_HG00118.bam
samtools view -F 0x0002 BRCA1_HG00118.bam
samtools view -F 0x0040 BRCA1_HG00118.bam
@ 
<<view.tips, engine = 'bash', eval = FALSE, results = results, echo = echo>>=
## Some short comments for each line below
samtools view -f 0x0002 BRCA1_HG00118.bam #prints properly mapped pairs
samtools view -F 0x0002 BRCA1_HG00118.bam #excludes properly mapped pairs
samtools view -F 0x0040 BRCA1_HG00118.bam #print first read of pair
@ 
\end{enumerate}


\subsection{Understanding the SAM/BAM headers (10 minutes)}

Key information about a BAM file can be obtained from the headers.
\texttt{Samtools} is once again the key tool to read these data.
You can view these headers using:
<<headers, engine = 'bash'>>=
samtools view -H BRCA1_HG00118.bam > headers.txt
@ 
<<headers.2, engine = 'bash', eval = FALSE>>=
less -S headers.txt
@ 

\noindent
{\bf Exercise:} Go through the output of \texttt{samtools view -H} and make sure you understand what all the fields mean.
This will be discussed in class. In the case of the downloaded 1KG file, much processing has happened so not all fields will make sense, but the important thing is to understand the general philosophy of the file. The ID, LB, SM tags are particularly useful.



\subsection{Using \texttt{R} and \texttt{Rsamtools} (20 minutes)}

The \texttt{Rsamtools} package in \texttt{R} is very effective to parse BAM files, and extremely memory efficient, making full used of BAM indexes.
Look at the example below for example.
Inspect the output object called \texttt{bam.reads}.
Can you understand its structure? See what it contains and how the data are organised?
We will be using these tools later on in the CNV analysis section.

<<Rsamtools>>=
library(Rsamtools)
library(GenomicRanges)

which <- GRanges(seqnames=Rle('21'),
                 IRanges(start = 43000000, end = 45000000))

what <- c("rname", "strand", "pos", "qwidth", "seq")
param <- ScanBamParam(which=which, what=what)

bam.reads <- scanBam(file = '../data/BAM_files/HG00251.mapped.ILLUMINA.bwa.GBR.exome.20121211.bam',
                     param=param)
names(bam.reads[[1]])
head(bam.reads[[1]]$pos)
@ 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section*{Session info}
<<sessionInfo>>=
sessionInfo()
@ 


\end{document}
