\documentclass[a4paper]{article}
\usepackage[colorlinks=true]{hyperref}
\usepackage{fullpage}

\title{BCGES short courses, session 1: Introduction, file formats, shell scripting, Galaxy}
\date{}
\author{Vincent Plagnol}

\begin{document}
%\SweaveOpts{concordance=TRUE}
%knit('practical.Rnw'); system('pdflatex practical')

\maketitle

<<setup, results = 'hide', echo = FALSE>>=
if (!exists('forStudents')) forStudents <- TRUE  ##default is to print the student version
if (!forStudents) {echo <- TRUE; results <- 'markup'} else {results <- 'hide'; echo  <- FALSE;}
@

The aim of this first session is to introduce the tools that we will use to manipulate sequence data.
Generally speaking, we will deal with either \texttt{R}, bash scripting or \href{https://wiki.galaxyproject.org/PublicGalaxyServers}{Galaxy}.


\texttt{R} is not usually the preferred way to deal with HTS data.
One reason for this is that the general approach of \texttt{R} is to load data into the RAM as a first instance, which is not practical when the datasets are very large.
Shell scripts (aka command line tools) that read the data on a line per line basis are usually preferred.
Nevertheless, \texttt{R} has developed tools to overcome these limitations and there is, in fact, surprisingly much that one can do with it.
It is also a practical tools for teaching purposes, which is useful in the context of these short courses.
So we will use \texttt{R} mostly to play with the data and show the sort of things one may want to do with it.

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Basic fastq reading and quality control}


\subsection{Basic shell scripting to read NGS files}


Many of the standard HTS formats are simple text files, with tightly defined specifications to allow effective parsing.
Some of these files can be compressed and/or indexed to enable quicker access.
The first esstential step is to be confortable with reading/compressing/uncompressing various text files.

<<head, engine='bash'>>=
head ../../data/fastq_files/fastq1_1.txt
@ 


<<tail, engine='bash'>>=
tail ../../data/fastq_files/fastq1_1.txt
@ 

If you need more information, shell scripts have useful manual page that can be accessed using the \texttt{man} command.
<<man, engine='bash', eval=FALSE>>=
man head
man tail
@ 

One can parse a text file using a variety of commands, and it is useful to become familiar with the following:
<<reads, engine='bash', eval = FALSE>>=
cat ../data/fastq_files/fastq1_1.txt
less ../data/fastq_files/fastq1_1.txt
more ../data/fastq_files/fastq1_1.txt
less -S ../data/fastq_files/fastq1_1.txt
@ 

\noindent
{\bf Exercise:} Can you see the differences between these various ways of reading a file?

\vspace{1.5cm}


A routine that is often useful is \texttt{wc} that counts the words/character/lines of a file. Try:
<<wc, engine='bash', eval= FALSE>>=
wc  ../data/fastq_files/fastq1_1.txt
wc  -l ../data/fastq_files/fastq1_1.txt
@


\noindent
{\bf Exercise:} Using the man page, find a way to print the first 20 lines of a fastq file (and what about the last 20 lines)?\\
{\bf Exercise:} Based on the \texttt{wc} output, how many reads do these fastq files contain?



\subsection{Using the shortRead package in R}
We start by loading one of the most relevant library, called ``ShortReads''. This package may not be installed but it can easily done so by running:
<<install, eval = FALSE>>=
source("http://bioconductor.org/biocLite.R")
biocLite("ShortRead")
@ 

<<load, results = 'hide', message = FALSE>>=
library('ShortRead')
@ 

As a starting point it is possible to read some of the examples fastq files and create relevant \texttt{R} objects.
<<readFastq, eval=TRUE>>=
fastq1.1 <- readFastq('../data/fastq_files/fastq1_1.txt')
fastq1.2 <- readFastq('../data/fastq_files/fastq1_2.txt')
@ 

We can now display the sequences and the qualities. Note that specific classes have been defined to store each of these objects.
A lot of work has gone into figuring out how to do this.

<<showFastq>>=
reads <- sread(fastq1.1)
class(reads)
head(as.character(reads))

ids <- id(fastq1.1)
class(ids)
head(as.character(ids))
@ 

It is also possible to use this package to look at quality scores.
For example, following up on the example above:

<<showQuals>>=
quals <- quality(fastq1.1)
class(quals)
quals
@ 


The \texttt{ShortRead} package will attempt to guess what these quality scores mean, for example see:
<<guessQual>>=
encoding(quality(fastq1.1))

fastq2.1 <- readFastq('../data/fastq_files/fastq2_1.txt')
encoding(quality(fastq2.1))
@ 

\noindent
{\bf (Optional, and somewhat difficult) Exercise:} Generate a plot showing the average Phred score as a function of the position in the read.

\subsection{Using the Galaxy server}

Start by identifying a working instance of the Galaxy server from this \href{https://wiki.galaxyproject.org/PublicGalaxyServers}{location}.
There are multiple choices here, so feel free to experiment.
In case of doubt, I used the following server: \href{http://biominavm-galaxy.biomina.be/galaxy/}{http://biominavm-galaxy.biomina.be/galaxy/}.

\noindent
{\bf Exercise:} Create an account on one of these Galaxy servers and upload the pair of fastq {\it fastq2\_1.txt} and {\it fastq2\_2.txt}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Merging overlapping with paired reads}

Another useful thing to be aware of is the possibility to merge read pairs that overlap in the middle.
This happens when the combined length of both reads exceeds the length of the DNA fragment they originate from.
A review of tools to perform this task is available \href{http://thegenomefactory.blogspot.de/2012/11/tools-to-merge-overlapping-paired-end.html}{here}.
I propose here to have a look at the program \href{http://sco.h-its.org/exelixis/web/software/pear/doc.html}{PEAR}, which has been useful for the applications I had in mind.
Start by downloading the pre-compiled binary to your own computer (this program is not preinstalled).
You should be able to run something like the command below, after changing the path to the appropriate location of the executable:

<<pear, engine = 'bash', eval = FALSE>>=
/cluster/project8/vyp/vincent/Software/pear_github/PEAR-master/src/pear -e -v 10 -f ../data/fastq_for_merging/reads1.fq -r ../data/fastq_for_merging/reads1.fq -o merged -c 40
@

Check what the output looks like, in particular the read length of the assembled reads. 
Have a look at the options that PEAR uses, and see how the choice of the v argument affects the final results.
Note that PEAR requires that you know which of the reads are forward and reverse, which in practice is not the case so I am not completely sure how the program would run on a standard non-stranded fastq file.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Reading BAM files}
BAM files are compressed files that contain the information from the FASTQ files, plus additional information about the location where the reads map.
A key feature of the BAM files is that they can be indexed, i.e. an associated file contains information about where each of the reads are located in the file.
It allows very quick retrieval of reads that map to a given genomic location, which is the typical way one wants to use BAM files (for example, to extract all the reads that map to a gene of interest in order to find rare variants).

\subsection{Using shell scripts}

\texttt{samtools} is the key piece of software that is used to read, write and index BAM files.
The \href{http://samtools.sourceforge.net/samtools.shtml}{manual page} is the first place to go to find information about how to use \texttt{samtools}.



\subsection{Using \texttt{R} and \texttt{Rsamtools}}

The \texttt{Rsamtools} package in \texttt{R} is very effective to parse BAM files, and extremely memory efficient, making full used of BAM indexes.
Look at the example below for example.
Inspect the output object called \texttt{bam.reads}.
Can you understand its structure? See what it contains and how the data are organised?
We will be using these tools later on in the CNV analysis section.

<<Rsamtools>>=
library(Rsamtools)
library(GenomicRanges)

which <- GRanges(seqnames=Rle('21'),
                 IRanges(start = 43000000, end = 45000000))

what <- c("rname", "strand", "pos", "qwidth", "seq")
param <- ScanBamParam(which=which, what=what)

bam.reads <- scanBam(file = '../data/BAM_files/HG00251.mapped.ILLUMINA.bwa.GBR.exome.20121211.bam',
                     param=param)
names(bam.reads[[1]])
@ 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Session info}
<<sessionInfo>>=
sessionInfo()
@ 


\end{document}
